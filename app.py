import streamlit as st

# Bu komut en baÅŸta olmalÄ± - diÄŸer streamlit komutlarÄ±ndan Ã¶nce
st.set_page_config(page_title="KapsamlÄ± CV Bias Analiz AracÄ±", layout="wide", 
                  page_icon="ğŸ“Š", initial_sidebar_state="expanded")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import os
import tempfile
import io
import re
import warnings
warnings.filterwarnings('ignore')

# Bias analiz kÃ¼tÃ¼phaneleri
try:
    from aif360.datasets import BinaryLabelDataset
    from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
    from aif360.algorithms.preprocessing import Reweighing, DisparateImpactRemover
    from aif360.algorithms.inprocessing import AdversarialDebiasing
    from aif360.algorithms.postprocessing import CalibratedEqualizedOdds
    aif360_available = True
except ImportError:
    aif360_available = False
    st.warning("AIF360 kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. Tam bias analizi iÃ§in: pip install aif360")

# Fairlearn entegrasyonu
try:
    from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference
    from fairlearn.reductions import ExponentiatedGradient, GridSearch
    from fairlearn.widget import FairlearnDashboard
    fairlearn_available = True
except ImportError:
    fairlearn_available = False
    st.warning("Fairlearn kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. Fairness metrikleri iÃ§in: pip install fairlearn")

# AÃ§Ä±klanabilirlik kÃ¼tÃ¼phaneleri
try:
    import shap
    import lime
    from lime.lime_tabular import LimeTabularExplainer
    import eli5
    from eli5.sklearn import PermutationImportance
    explainability_available = True
except ImportError:
    explainability_available = False
    st.warning("SHAP, LIME veya ELI5 kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. Model aÃ§Ä±klanabilirliÄŸi iÃ§in bunlarÄ± yÃ¼kleyin.")

# PDF ve DOCX iÅŸleme kÃ¼tÃ¼phaneleri
try:
    import PyPDF2
    import docx
    pdf_docx_support = True
except ImportError:
    pdf_docx_support = False
    st.warning("PDF/DOCX desteÄŸi eksik. PDF ve DOCX dosyalarÄ±nÄ± iÅŸlemek iÃ§in: pip install PyPDF2 python-docx")


# Uygulama baÅŸlÄ±ÄŸÄ± ve aÃ§Ä±klama
st.title("ğŸ“Š KapsamlÄ± CV Bias Analiz AracÄ±")
st.write("""
Bu geliÅŸmiÅŸ araÃ§, Ã¶zgeÃ§miÅŸ tarama ve iÅŸe alÄ±m sÃ¼reÃ§lerindeki bias (yanlÄ±lÄ±k) ve fairness (adillik) sorunlarÄ±nÄ± tespit etmenize, 
analiz etmenize ve azaltmanÄ±za yardÄ±mcÄ± olur. EU AI Act kapsamÄ±nda iÅŸe alÄ±m sistemleri yÃ¼ksek riskli AI uygulamalarÄ± 
arasÄ±nda deÄŸerlendirilmektedir.

**Desteklenen Formatlar:** CSV, PDF, DOCX ve TXT
""")

# ---- YardÄ±mcÄ± Fonksiyonlar ----
def extract_text_from_pdf(file_obj):
    """PDF dosyasÄ±ndan metin Ã§Ä±karÄ±r"""
    try:
        pdf_reader = PyPDF2.PdfReader(file_obj)
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()
        return text
    except Exception as e:
        st.error(f"PDF iÅŸleme hatasÄ±: {str(e)}")
        return ""

def extract_text_from_docx(file_obj):
    """DOCX dosyasÄ±ndan metin Ã§Ä±karÄ±r"""
    try:
        doc = docx.Document(file_obj)
        text = []
        for para in doc.paragraphs:
            text.append(para.text)
        return '\n'.join(text)
    except Exception as e:
        st.error(f"DOCX iÅŸleme hatasÄ±: {str(e)}")
        return ""

def extract_demographics(text):
    """Metinden demografik Ã¶zellikleri Ã§Ä±karÄ±r"""
    demographics = {}
    
    # Cinsiyet tespiti
    male_keywords = ['bay', 'erkek', 'adam', 'bey', 'mr', 'male', 'he', 'his']
    female_keywords = ['bayan', 'kadÄ±n', 'hanÄ±m', 'ms', 'mrs', 'female', 'she', 'her']
    
    male_count = sum([1 for word in male_keywords if word.lower() in text.lower()])
    female_count = sum([1 for word in female_keywords if word.lower() in text.lower()])
    
    if male_count > female_count:
        demographics['cinsiyet'] = 'Erkek'
    elif female_count > male_count:
        demographics['cinsiyet'] = 'KadÄ±n'
    else:
        demographics['cinsiyet'] = 'Bilinmiyor'
    
    # EÄŸitim seviyesi
    education_levels = {
        'Doktora': ['doktora', 'phd', 'ph.d', 'doktor', 'dr.'],
        'YÃ¼ksek Lisans': ['yÃ¼ksek lisans', 'master', 'msc', 'm.sc', 'mba'],
        'Lisans': ['lisans', 'Ã¼niversite', 'fakÃ¼lte', 'bsc', 'b.sc', 'bachelor'],
        'Ã–n Lisans': ['Ã¶n lisans', 'yÃ¼ksekokul', 'meslek yÃ¼ksekokul', 'associate'],
        'Lise': ['lise', 'high school', 'orta Ã¶ÄŸretim']
    }
    
    for level, keywords in education_levels.items():
        if any(keyword.lower() in text.lower() for keyword in keywords):
            demographics['egitim'] = level
            break
    else:
        demographics['egitim'] = 'Bilinmiyor'
    
    # YaÅŸ/deneyim tespiti
    experience_match = re.search(r'(\d+)\s*(?:yÄ±l|year|sene)(?:\s+deneyim|experience)?', text.lower())
    if experience_match:
        experience = int(experience_match.group(1))
        demographics['deneyim_yil'] = experience
    else:
        demographics['deneyim_yil'] = 0
    
    # YaÅŸ tespiti
    age_match = re.search(r'(?:yaÅŸ|age|years old)\s*:?\s*(\d+)', text.lower())
    if age_match:
        age = int(age_match.group(1))
        if 20 <= age <= 25:
            demographics['yas_grubu'] = '20-25'
        elif 26 <= age <= 30:
            demographics['yas_grubu'] = '26-30'
        elif 31 <= age <= 35:
            demographics['yas_grubu'] = '31-35'
        elif 36 <= age <= 40:
            demographics['yas_grubu'] = '36-40'
        elif 41 <= age <= 50:
            demographics['yas_grubu'] = '41-50'
        elif age > 50:
            demographics['yas_grubu'] = '51+'
        else:
            demographics['yas_grubu'] = 'Bilinmiyor'
    else:
        # Deneyime gÃ¶re yaÅŸ tahmini
        if demographics['deneyim_yil'] > 0:
            est_age = demographics['deneyim_yil'] + 22  # Ãœniversite mezuniyeti + deneyim
            if 20 <= est_age <= 25:
                demographics['yas_grubu'] = '20-25'
            elif 26 <= est_age <= 30:
                demographics['yas_grubu'] = '26-30'
            elif 31 <= est_age <= 35:
                demographics['yas_grubu'] = '31-35'
            elif 36 <= est_age <= 40:
                demographics['yas_grubu'] = '36-40'
            elif 41 <= est_age <= 50:
                demographics['yas_grubu'] = '41-50'
            elif est_age > 50:
                demographics['yas_grubu'] = '51+'
            else:
                demographics['yas_grubu'] = 'Bilinmiyor'
        else:
            demographics['yas_grubu'] = 'Bilinmiyor'
    
    # Dil yetenekleri
    language_keywords = ['ingilizce', 'almanca', 'fransÄ±zca', 'ispanyolca', 'italyanca', 'rusÃ§a', 'Ã§ince', 'japonca',
                        'english', 'german', 'french', 'spanish', 'italian', 'russian', 'chinese', 'japanese']
    language_count = sum([1 for word in language_keywords if word.lower() in text.lower()])
    demographics['dil_sayisi'] = min(max(language_count, 1), 5)  # En az 1, en fazla 5 dil
    
    return demographics

def calculate_cv_score(demographics):
    """Demografik bilgilere gÃ¶re CV skoru hesaplar"""
    cv_score = 50  # BaÅŸlangÄ±Ã§ puanÄ±
    
    # EÄŸitim puanÄ±
    if demographics['egitim'] == 'Doktora':
        cv_score += 20
    elif demographics['egitim'] == 'YÃ¼ksek Lisans':
        cv_score += 15
    elif demographics['egitim'] == 'Lisans':
        cv_score += 10
    elif demographics['egitim'] == 'Ã–n Lisans':
        cv_score += 5
    
    # Deneyim puanÄ±
    cv_score += min(demographics['deneyim_yil'] * 2, 30)
    
    # Dil puanÄ±
    cv_score += demographics['dil_sayisi'] * 3
    
    # YaÅŸa gÃ¶re (genÃ§ yaÅŸ grubuna bonus - ageism)
    if demographics['yas_grubu'] in ['20-25', '26-30', '31-35']:
        cv_score += 5
    elif demographics['yas_grubu'] in ['41-50', '51+']:
        cv_score -= 5
    
    # Cinsiyete gÃ¶re eÅŸit puan ekleme (sektÃ¶r bias'Ä±nÄ± temsil etmek iÃ§in)
    # GerÃ§ek uygulamada bu tÃ¼r bias'lar ortadan kaldÄ±rÄ±lmalÄ±dÄ±r
    if demographics['cinsiyet'] == 'Erkek':
        cv_score += 3  # Erkeklere teknoloji sektÃ¶rÃ¼ndeki tarihi bias'Ä± simÃ¼le etmek iÃ§in
    
    # Skor sÄ±nÄ±rlandÄ±rmasÄ±
    cv_score = max(min(cv_score, 100), 0)
    
    return round(cv_score, 2)

def create_sample_cv_data(num_records=1000, seed=42):
    """Ã–rnek CV verileri oluÅŸturur"""
    np.random.seed(seed)
    
    # Demografik bilgiler
    genders = ['Erkek', 'KadÄ±n']
    age_groups = ['20-25', '26-30', '31-35', '36-40', '41-50', '51+']
    education_levels = ['Lise', 'Ã–n Lisans', 'Lisans', 'YÃ¼ksek Lisans', 'Doktora']
    positions = ['YazÄ±lÄ±m GeliÅŸtirici', 'Veri Bilimci', 'Pazarlama UzmanÄ±', 'SatÄ±ÅŸ Temsilcisi', 'Ä°nsan KaynaklarÄ±']
    
    # Bias ekleyelim - iÅŸe alÄ±m oranlarÄ±nda
    gender_bias = {'Erkek': 0.60, 'KadÄ±n': 0.40}  # Erkekler lehine
    age_bias = {'20-25': 0.55, '26-30': 0.65, '31-35': 0.60, '36-40': 0.50, '41-50': 0.40, '51+': 0.30}  # GenÃ§ adaylar lehine
    
    data = []
    for _ in range(num_records):
        gender = np.random.choice(genders, p=[0.6, 0.4])  # Cinsiyet daÄŸÄ±lÄ±mÄ±
        age_group = np.random.choice(age_groups, p=[0.15, 0.25, 0.25, 0.15, 0.15, 0.05])  # YaÅŸ daÄŸÄ±lÄ±mÄ±
        education = np.random.choice(education_levels, p=[0.1, 0.15, 0.45, 0.25, 0.05])  # EÄŸitim daÄŸÄ±lÄ±mÄ±
        position = np.random.choice(positions, p=[0.3, 0.2, 0.2, 0.15, 0.15])  # Pozisyon daÄŸÄ±lÄ±mÄ±
        
        # Ã–zellik deÄŸerleri
        years_experience = np.random.randint(0, 20)
        languages = np.random.randint(1, 5)
        
        # Ã–rnek AI tarafÄ±ndan hesaplanan skor (iÃ§inde bias var)
        base_score = np.random.normal(70, 15)
        
        # Bias faktÃ¶rlerini ekleyelim
        if gender == 'Erkek':
            base_score += np.random.normal(5, 2)  # Erkeklere bonus
        
        if age_group in ['20-25', '26-30', '31-35']:
            base_score += np.random.normal(4, 2)  # GenÃ§lere bonus
        elif age_group in ['41-50', '51+']:
            base_score += np.random.normal(-5, 2)  # YaÅŸlÄ±lara penaltÄ±
        
        # EÄŸitim dÃ¼zeyine gÃ¶re bonus
        if education == 'Lisans':
            base_score += np.random.normal(3, 1)
        elif education == 'YÃ¼ksek Lisans':
            base_score += np.random.normal(5, 1)
        elif education == 'Doktora':
            base_score += np.random.normal(7, 1)
            
        # Skor sÄ±nÄ±rlandÄ±rmasÄ±
        cv_score = max(min(base_score, 100), 0)
        
        # Ä°ÅŸe alÄ±m kararÄ± (bias iÃ§erir)
        hire_prob = (cv_score / 100) * gender_bias.get(gender, 0.5) * age_bias.get(age_group, 0.5)
        hired = 1 if np.random.random() < hire_prob else 0
            
        data.append({
            'cinsiyet': gender,
            'yas_grubu': age_group,
            'egitim': education,
            'pozisyon': position,
            'deneyim_yil': years_experience,
            'dil_sayisi': languages,
            'cv_skoru': round(cv_score, 2),
            'ise_alindi': hired
        })
    
    return pd.DataFrame(data)

def calculate_disparate_impact(df, protected_attribute, target_col):
    """Bias metriklerini hesaplar ve rapor oluÅŸturur"""
    group_means = df.groupby(protected_attribute)[target_col].mean().reset_index()
    
    # Referans grup (ilk grup)
    reference_group = group_means[protected_attribute].iloc[0]
    reference_rate = group_means[target_col].iloc[0]
    
    # Bias metrikleri hesapla
    disparate_impacts = []
    for i, row in group_means.iterrows():
        group = row[protected_attribute]
        rate = row[target_col]
        
        if i == 0:  # Referans grup
            di = 1.0
            spd = 0.0
        else:  # DiÄŸer gruplar
            di = rate / reference_rate if reference_rate > 0 else float('inf')
            spd = rate - reference_rate  # Statistical Parity Difference
        
        eod = 0.0  # Equal Opportunity Difference - GeliÅŸmiÅŸ analizde hesaplanÄ±r
        
        disparate_impacts.append({
            'Grup': group,
            'Ä°ÅŸe AlÄ±m OranÄ±': rate,
            'Disparate Impact': di,
            'Statistical Parity Diff': spd,
            'Equal Opportunity Diff': eod,
            'Bias Tespit Edildi': (di < 0.8 or di > 1.2)
        })
    
    return pd.DataFrame(disparate_impacts)

def calculate_fairness_metrics(df, protected_attribute, target_col, model=None, X=None, y=None):
    """AIF360 kullanarak geliÅŸmiÅŸ fairness metrikleri hesaplar"""
    if not aif360_available:
        return None, "AIF360 kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. pip install aif360 komutuyla yÃ¼kleyebilirsiniz."
    
    try:
        # Korunan Ã¶zellik ve hedef deÄŸiÅŸken iÃ§in binary dataset oluÅŸtur
        privileged_groups = [{protected_attribute: df[protected_attribute].value_counts().index[0]}]
        unprivileged_groups = [{protected_attribute: df[protected_attribute].value_counts().index[1]}]
        
        dataset = BinaryLabelDataset(
            df=df,
            label_names=[target_col],
            protected_attribute_names=[protected_attribute],
            privileged_protected_attributes=[1]
        )
        
        # Bias metrikleri hesapla
        metrics = BinaryLabelDatasetMetric(dataset, 
                                        unprivileged_groups=unprivileged_groups,
                                        privileged_groups=privileged_groups)
        
        disparate_impact = metrics.disparate_impact()
        statistical_parity_difference = metrics.statistical_parity_difference()
        
        # EÄŸer model varsa, sÄ±nÄ±flandÄ±rma metriklerini hesapla
        classification_metrics = {}
        if model is not None and X is not None and y is not None:
            # Model predictions
            y_pred = model.predict(X)
            
            # Korunan gruplara gÃ¶re doÄŸruluk
            accuracy_privileged = accuracy_score(
                y[df[protected_attribute] == privileged_groups[0][protected_attribute]], 
                y_pred[df[protected_attribute] == privileged_groups[0][protected_attribute]]
            )
            
            accuracy_unprivileged = accuracy_score(
                y[df[protected_attribute] == unprivileged_groups[0][protected_attribute]], 
                y_pred[df[protected_attribute] == unprivileged_groups[0][protected_attribute]]
            )
            
            classification_metrics = {
                'accuracy_privileged': accuracy_privileged,
                'accuracy_unprivileged': accuracy_unprivileged,
                'accuracy_difference': accuracy_privileged - accuracy_unprivileged
            }
        
        fairness_metrics = {
            'disparate_impact': disparate_impact,
            'statistical_parity_difference': statistical_parity_difference,
            'classification_metrics': classification_metrics
        }
        
        return fairness_metrics, None
    
    except Exception as e:
        return None, f"Fairness metrikleri hesaplanÄ±rken hata oluÅŸtu: {str(e)}"

def apply_bias_mitigation(df, protected_attribute, target_col, method="reweighing"):
    """AIF360 kullanarak bias azaltma algoritmalarÄ± uygular"""
    if not aif360_available:
        return None, "AIF360 kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. pip install aif360 komutuyla yÃ¼kleyebilirsiniz."
    
    try:
        # Korunan Ã¶zellik ve hedef deÄŸiÅŸken iÃ§in binary dataset oluÅŸtur
        privileged_groups = [{protected_attribute: df[protected_attribute].value_counts().index[0]}]
        unprivileged_groups = [{protected_attribute: df[protected_attribute].value_counts().index[1]}]
        
        dataset = BinaryLabelDataset(
            df=df,
            label_names=[target_col],
            protected_attribute_names=[protected_attribute],
            privileged_protected_attributes=[1]
        )
        
        # Bias azaltma algoritmasÄ± seÃ§ ve uygula
        if method == "reweighing":
            mitigator = Reweighing(unprivileged_groups=unprivileged_groups,
                                privileged_groups=privileged_groups)
            transformed_dataset = mitigator.fit_transform(dataset)
        
        elif method == "disparate_impact_remover":
            mitigator = DisparateImpactRemover(repair_level=0.8)
            transformed_dataset = mitigator.fit_transform(dataset)
        
        else:
            return None, f"Desteklenmeyen bias azaltma yÃ¶ntemi: {method}"
        
        # DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ veriyi DataFrame'e Ã§evir
        transformed_df = transformed_dataset.convert_to_dataframe()[0]
        
        return transformed_df, None
    
    except Exception as e:
        return None, f"Bias azaltma uygulanÄ±rken hata oluÅŸtu: {str(e)}"

def train_model(df, protected_attribute, target_col):
    """CV verileri Ã¼zerinde model eÄŸitir ve model performans metriklerini hesaplar"""
    # EÄŸitim ve test verilerini ayÄ±r
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # Kategorik deÄŸiÅŸkenleri one-hot encoding
    X = pd.get_dummies(X)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Model eÄŸitimi
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Model performansÄ±nÄ± deÄŸerlendirme
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    
    # Korunan Ã¶zelliÄŸe gÃ¶re model performansÄ±
    if protected_attribute in df.columns:
        group_performance = {}
        protected_col_onehot = [col for col in X.columns if protected_attribute in col]
        
        if len(protected_col_onehot) > 0:
            # One-hot encoded sÃ¼tunlardan orijinal korunan Ã¶zellik deÄŸerlerini geri Ã§Ä±karmak
            # iÃ§in X_test'e bakÄ±p gruplarÄ± belirlemek gerekir - basitleÅŸtirmek iÃ§in burada atlanÄ±yor
            pass
        
        # Gruplara gÃ¶re performans Ã¶lÃ§Ã¼mleri
        for group in df[protected_attribute].unique():
            group_mask = df.loc[X_test.index, protected_attribute] == group
            if sum(group_mask) > 0:
                group_y_test = y_test[group_mask]
                group_y_pred = y_pred[group_mask]
                group_performance[group] = {
                    'accuracy': accuracy_score(group_y_test, group_y_pred),
                    'precision': sum((group_y_pred == 1) & (group_y_test == 1)) / sum(group_y_pred == 1) if sum(group_y_pred == 1) > 0 else 0,
                    'recall': sum((group_y_pred == 1) & (group_y_test == 1)) / sum(group_y_test == 1) if sum(group_y_test == 1) > 0 else 0
                }
    
    return model, X_test, y_test, y_pred, accuracy, cm, report, group_performance

def analyze_feature_importance(model, X):
    """Model Ã¶zellik Ã¶nemini analiz eder"""
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return feature_importance

def generate_shap_values(model, X):
    """SHAP deÄŸerlerini hesaplar (aÃ§Ä±klanabilirlik)"""
    if not explainability_available:
        return None, "SHAP kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. pip install shap komutuyla yÃ¼kleyebilirsiniz."
    
    try:
        # SHAP deÄŸerleri hesapla
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X)
        
        return shap_values, explainer, None
    
    except Exception as e:
        return None, None, f"SHAP deÄŸerleri hesaplanÄ±rken hata oluÅŸtu: {str(e)}"

def prepare_lime_explainer(X_train, feature_names):
    """LIME aÃ§Ä±klayÄ±cÄ± oluÅŸturur"""
    if not explainability_available:
        return None, "LIME kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. pip install lime komutuyla yÃ¼kleyebilirsiniz."
    
    try:
        # LIME aÃ§Ä±klayÄ±cÄ± oluÅŸtur
        explainer = LimeTabularExplainer(
            X_train.values, 
            feature_names=feature_names, 
            class_names=['Reddedildi', 'Ä°ÅŸe AlÄ±ndÄ±'],
            mode='classification'
        )
        
        return explainer, None
    
    except Exception as e:
        return None, f"LIME aÃ§Ä±klayÄ±cÄ± oluÅŸturulurken hata oluÅŸtu: {str(e)}"

# ---- Ana Uygulama ----
# Sidebar
st.sidebar.header("ğŸ“Š Analiz Parametreleri")

# Veri kaynaÄŸÄ± seÃ§enekleri
data_option = st.sidebar.radio("Veri kaynaÄŸÄ±:", ["CV YÃ¼kle", "Ã–rnek Veri"])

# Dosya yÃ¼kleme
if data_option == "CV YÃ¼kle":
    # PDF/DOCX destek kontrolÃ¼
    if not pdf_docx_support:
        st.sidebar.warning("PDF ve DOCX destekleri eksik. pip install PyPDF2 python-docx")
        file_types = ["csv"]
        accept_type = "CSV dosyanÄ±zÄ± yÃ¼kleyin"
    else:
        file_types = ["csv", "pdf", "docx", "doc", "txt"]
        accept_type = "CV dosyalarÄ±nÄ±zÄ± yÃ¼kleyin"
    
    uploaded_files = st.sidebar.file_uploader(accept_type, type=file_types, accept_multiple_files=True)

# Ã–rnek veri parametreleri
if data_option == "Ã–rnek Veri" or (data_option == "CV YÃ¼kle" and not uploaded_files):
    sample_size = st.sidebar.slider("Ã–rnek veri sayÄ±sÄ±", 100, 5000, 1000, 100)
    if data_option == "CV YÃ¼kle" and not uploaded_files:
        st.sidebar.info("Dosya yÃ¼klenmedi. Ã–rnek veri kullanÄ±lacak.")

# Analiz parametreleri
st.sidebar.subheader("Korunan Ã–zellik ve Hedef")
protected_attribute = st.sidebar.selectbox(
    "Korunan Ã¶zelliÄŸi seÃ§in", 
    ["cinsiyet", "yas_grubu", "egitim"],
    index=0
)

target_col = st.sidebar.selectbox(
    "Hedef deÄŸiÅŸkeni seÃ§in",
    ["ise_alindi"],
    index=0
)

# Bias azaltma yÃ¶ntemi (AIF360 varsa)
st.sidebar.subheader("Bias Azaltma")
if aif360_available:
    apply_mitigation = st.sidebar.checkbox("Bias azaltma uygula", value=False)
    if apply_mitigation:
        mitigation_method = st.sidebar.selectbox(
            "Bias azaltma yÃ¶ntemi",
            ["reweighing", "disparate_impact_remover"],
            index=0
        )
else:
    apply_mitigation = False
    st.sidebar.warning("Bias azaltma iÃ§in AIF360 gerekli")

# Analiz butonu
analyze_button = st.sidebar.button("Analiz Et", type="primary")

# Model ve aÃ§Ä±klanabilirlik ayarlarÄ±
st.sidebar.subheader("Model Analizi")
model_type = st.sidebar.selectbox(
    "Model tÃ¼rÃ¼",
    ["Random Forest", "Lojistik Regresyon"],
    index=0
)

if explainability_available:
    st.sidebar.subheader("AÃ§Ä±klanabilirlik")
    explainability_method = st.sidebar.multiselect(
        "AÃ§Ä±klanabilirlik yÃ¶ntemleri",
        ["SHAP", "LIME", "ELI5"],
        default=["SHAP"]
    )
else:
    explainability_method = []
    st.sidebar.warning("AÃ§Ä±klanabilirlik iÃ§in SHAP, LIME ve ELI5 gerekli")

# Sekmeleri oluÅŸtur
tabs = st.tabs(["Veri Analizi", "Bias Analizi", "Model Analizi", "AÃ§Ä±klanabilirlik", "EU AI Act Rehberi", "Deployment"])

# Veri YÃ¼kleme veya OluÅŸturma
if data_option == "CV YÃ¼kle" and uploaded_files:
    cv_data = []
    
    for uploaded_file in uploaded_files:
        file_ext = uploaded_file.name.split('.')[-1].lower()
        
        if file_ext == 'csv':
            # CSV dosyasÄ±nÄ± doÄŸrudan oku
            df = pd.read_csv(uploaded_file)
            st.success(f"CSV dosyasÄ± baÅŸarÄ±yla yÃ¼klendi: {uploaded_file.name}")
            break  # CSV dosyasÄ± bulunduÄŸunda diÄŸer dosyalarÄ± iÅŸlemeyi durdur
        
        elif pdf_docx_support:
            # PDF, DOCX ve TXT dosyalarÄ± iÃ§in metin Ã§Ä±karma
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp_file:
                tmp_file.write(uploaded_file.getbuffer())
                tmp_path = tmp_file.name
            
            text_content = ""
            try:
                if file_ext == 'pdf':
                    with open(tmp_path, 'rb') as f:
                        text_content = extract_text_from_pdf(f)
                elif file_ext in ['docx', 'doc']:
                    with open(tmp_path, 'rb') as f:
                        text_content = extract_text_from_docx(f)
                elif file_ext == 'txt':
                    with open(tmp_path, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                        
                # Demografik Ã¶zellikleri Ã§Ä±kar
                demographics = extract_demographics(text_content)
                
                # CV skoru hesapla
                cv_score = calculate_cv_score(demographics)
                
                # Ä°ÅŸe alÄ±m kararÄ± (Ã¶rnek)
                hired = 1 if cv_score > 65 else 0
                
                # Veri dizisine ekle
                cv_data.append({
                    'dosya_adi': uploaded_file.name,
                    'cinsiyet': demographics['cinsiyet'],
                    'yas_grubu': demographics['yas_grubu'],
                    'egitim': demographics['egitim'],
                    'deneyim_yil': demographics['deneyim_yil'],
                    'dil_sayisi': demographics.get('dil_sayisi', 1),
                    'cv_skoru': cv_score,
                    'ise_alindi': hired
                })
                
                st.success(f"Dosya iÅŸlendi: {uploaded_file.name}")
                
            except Exception as e:
                st.error(f"Dosya iÅŸleme hatasÄ± ({uploaded_file.name}): {str(e)}")
            
            finally:
                # GeÃ§ici dosyayÄ± temizle
                try:
                    os.unlink(tmp_path)
                except:
                    pass
    
    if cv_data:  # EÄŸer CV'lerden veri Ã§Ä±karÄ±ldÄ±ysa
        df = pd.DataFrame(cv_data)
    elif 'df' not in locals():  # HiÃ§ CSV yÃ¼klenmemiÅŸ ve CV'lerden veri Ã§Ä±karÄ±lamamÄ±ÅŸsa
        st.warning("HiÃ§bir dosya iÅŸlenemedi. Ã–rnek veri kullanÄ±lacak.")
        df = create_sample_cv_data(num_records=sample_size)

else:  # Ã–rnek veri kullan
    df = create_sample_cv_data(num_records=sample_size)

# Veri Analizi Sekmesi
with tabs[0]:
    st.header("ğŸ“Š Veri Analizi")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("Veri Ã–zeti")
        df_info = pd.DataFrame({
            'SÃ¼tun': df.columns,
            'Veri Tipi': df.dtypes,
            'BoÅŸ DeÄŸer': df.isnull().sum(),
            'Benzersiz DeÄŸer': [df[col].nunique() for col in df.columns]
        })
        st.dataframe(df_info)
        
        st.subheader("Veri Ã–nizleme")
        st.dataframe(df.head())
    
    with col2:
        st.subheader("Temel Ä°statistikler")
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        st.dataframe(df[numeric_cols].describe())
        
        st.subheader("Korelasyon Matrisi")
        corr = df[numeric_cols].corr()
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)
        st.pyplot(fig)
    
    # Demografik daÄŸÄ±lÄ±mlar
    st.subheader("Demografik DaÄŸÄ±lÄ±mlar")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        # Cinsiyet daÄŸÄ±lÄ±mÄ±
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.countplot(x='cinsiyet', data=df, ax=ax)
        ax.set_title('Cinsiyet DaÄŸÄ±lÄ±mÄ±')
        st.pyplot(fig)
        
        # EÄŸitim daÄŸÄ±lÄ±mÄ±
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.countplot(x='egitim', data=df, ax=ax)
        ax.set_title('EÄŸitim Seviyesi DaÄŸÄ±lÄ±mÄ±')
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
        st.pyplot(fig)
    
    with col2:
        # YaÅŸ grubu daÄŸÄ±lÄ±mÄ±
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.countplot(x='yas_grubu', data=df, ax=ax)
        ax.set_title('YaÅŸ Grubu DaÄŸÄ±lÄ±mÄ±')
        st.pyplot(fig)
        
        # Ä°ÅŸe alÄ±m oranlarÄ±
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.countplot(x='ise_alindi', data=df, ax=ax)
        ax.set_title('Ä°ÅŸe AlÄ±m DaÄŸÄ±lÄ±mÄ±')
        ax.set_xticklabels(['Reddedilen', 'Ä°ÅŸe AlÄ±nan'])
        st.pyplot(fig)
    
    # CV skoru daÄŸÄ±lÄ±mÄ±
    st.subheader("CV Skoru DaÄŸÄ±lÄ±mÄ±")
    
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.histplot(df['cv_skoru'], bins=20, kde=True, ax=ax)
    ax.set_title('CV Skoru DaÄŸÄ±lÄ±mÄ±')
    ax.set_xlabel('CV Skoru')
    ax.set_ylabel('Frekans')
    st.pyplot(fig)

# Bias Analizi Sekmesi
if analyze_button:
    with tabs[1]:
        st.header("âš–ï¸ Bias Analizi")
        
        # Demografik DaÄŸÄ±lÄ±m
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("Demografik DaÄŸÄ±lÄ±m")
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.countplot(x=protected_attribute, hue=target_col, data=df, ax=ax)
            ax.set_title(f"{protected_attribute.capitalize()} Demografik DaÄŸÄ±lÄ±mÄ±")
            ax.set_xlabel(protected_attribute.capitalize())
            ax.set_ylabel("KiÅŸi SayÄ±sÄ±")
            if target_col == 'ise_alindi':
                ax.legend(['Reddedilen', 'Ä°ÅŸe AlÄ±nan'])
            st.pyplot(fig)
        
        with col2:
            st.subheader("Ä°ÅŸe AlÄ±m OranlarÄ±")
            hire_rates = df.groupby(protected_attribute)[target_col].mean().reset_index()
            
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.barplot(x=protected_attribute, y=target_col, data=hire_rates, ax=ax)
            ax.set_title(f"{protected_attribute.capitalize()}'e GÃ¶re Ä°ÅŸe AlÄ±m OranlarÄ±")
            ax.set_xlabel(protected_attribute.capitalize())
            ax.set_ylabel("Ä°ÅŸe AlÄ±m OranÄ±")
            for i, v in enumerate(hire_rates[target_col]):
                ax.text(i, v + 0.02, f"{v:.2f}", ha='center')
            st.pyplot(fig)
        
        # Disparate Impact analizi
        st.subheader("Disparate Impact Analizi")
        
        di_results = calculate_disparate_impact(df, protected_attribute, target_col)
        
        col3, col4 = st.columns([1, 1])
        
        with col3:
            # SonuÃ§larÄ± renklendirerek gÃ¶ster
            def highlight_bias(val):
                if val == True:
                    return 'background-color: #ffcccb'  # Light red
                else:
                    return ''
            
            st.dataframe(di_results.style.applymap(highlight_bias, subset=['Bias Tespit Edildi']))
            
            # Bias bilgilendirmesi
            if di_results['Bias Tespit Edildi'].any():
                st.error("âš ï¸ Bias tespit edildi! EU AI Act kapsamÄ±nda dÃ¼zeltici Ã¶nlemler gerekli.")
            else:
                st.success("âœ… Bias tespit edilmedi. DeÄŸerler kabul edilebilir aralÄ±kta.")
        
        with col4:
            # Disparate Impact GrafiÄŸi
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.barplot(x='Grup', y='Disparate Impact', data=di_results, ax=ax)
            
            # 0.8 ve 1.2 sÄ±nÄ±rlarÄ±nÄ± gÃ¶ster
            ax.axhline(y=0.8, color='r', linestyle='--', label='Alt SÄ±nÄ±r (0.8)')
            ax.axhline(y=1.2, color='r', linestyle='--', label='Ãœst SÄ±nÄ±r (1.2)')
            ax.axhline(y=1.0, color='g', linestyle='-', label='EÅŸitlik (1.0)')
            
            ax.set_title("Disparate Impact Analizi")
            ax.set_xlabel(protected_attribute.capitalize())
            ax.set_ylabel("Disparate Impact")
            ax.legend()
            st.pyplot(fig)
        
        # CV SkorlarÄ± Analizi
        st.subheader("CV SkorlarÄ± DaÄŸÄ±lÄ±mÄ±")
        
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.boxplot(x=protected_attribute, y='cv_skoru', data=df, ax=ax)
        ax.set_title(f"{protected_attribute.capitalize()} GruplarÄ±na GÃ¶re CV SkorlarÄ±")
        ax.set_xlabel(protected_attribute.capitalize())
        ax.set_ylabel("CV Skoru")
        st.pyplot(fig)
        
        # IBM AI Fairness 360 Metrikleri (eÄŸer mevcut ise)
        st.subheader("IBM AI Fairness 360 Metrikleri")
        
        if aif360_available:
            fairness_metrics, error_msg = calculate_fairness_metrics(df, protected_attribute, target_col)
            
            if fairness_metrics:
                col5, col6 = st.columns([1, 1])
                
                with col5:
                    st.info(f"""
                    **Disparate Impact:** {fairness_metrics['disparate_impact']:.4f}  
                    (Hedef: 0.8 - 1.2 arasÄ±)
                    
                    **Statistical Parity Difference:** {fairness_metrics['statistical_parity_difference']:.4f}  
                    (Hedef: 0'a yakÄ±n deÄŸer)
                    """)
                
                with col6:
                    if fairness_metrics['classification_metrics']:
                        cm = fairness_metrics['classification_metrics']
                        st.info(f"""
                        **AyrÄ±calÄ±klÄ± Grup DoÄŸruluÄŸu:** {cm['accuracy_privileged']:.4f}
                        
                        **DezavantajlÄ± Grup DoÄŸruluÄŸu:** {cm['accuracy_unprivileged']:.4f}
                        
                        **DoÄŸruluk FarkÄ±:** {cm['accuracy_difference']:.4f}
                        """)
            else:
                st.warning(error_msg)
        else:
            st.warning("IBM AI Fairness 360 kÃ¼tÃ¼phanesi yÃ¼klenmemiÅŸ. GeliÅŸmiÅŸ metrikler iÃ§in AIF360 gerekli.")
        
        # Bias Azaltma (eÄŸer seÃ§ilmiÅŸse)
        if apply_mitigation and aif360_available:
            st.subheader("Bias Azaltma SonuÃ§larÄ±")
            
            transformed_df, error_msg = apply_bias_mitigation(df, protected_attribute, target_col, method=mitigation_method)
            
            if transformed_df is not None:
                col7, col8 = st.columns([1, 1])
                
                with col7:
                    st.subheader("Azaltma Ã–ncesi Ä°ÅŸe AlÄ±m OranlarÄ±")
                    before_rates = df.groupby(protected_attribute)[target_col].mean().reset_index()
                    
                    fig, ax = plt.subplots(figsize=(8, 6))
                    sns.barplot(x=protected_attribute, y=target_col, data=before_rates, ax=ax)
                    ax.set_title("Azaltma Ã–ncesi")
                    ax.set_ylim(0, 1)
                    st.pyplot(fig)
                
                with col8:
                    st.subheader("Azaltma SonrasÄ± Ä°ÅŸe AlÄ±m OranlarÄ±")
                    after_rates = transformed_df.groupby(protected_attribute)[target_col].mean().reset_index()
                    
                    fig, ax = plt.subplots(figsize=(8, 6))
                    sns.barplot(x=protected_attribute, y=target_col, data=after_rates, ax=ax)
                    ax.set_title(f"Azaltma SonrasÄ± ({mitigation_method})")
                    ax.set_ylim(0, 1)
                    st.pyplot(fig)
                
                # Bias metrikleri deÄŸiÅŸimi
                before_di = calculate_disparate_impact(df, protected_attribute, target_col)
                after_di = calculate_disparate_impact(transformed_df, protected_attribute, target_col)
                
                st.subheader("Bias Metriklerindeki DeÄŸiÅŸim")
                
                col9, col10 = st.columns([1, 1])
                
                with col9:
                    st.markdown("**Azaltma Ã–ncesi Disparate Impact:**")
                    st.dataframe(before_di[['Grup', 'Disparate Impact', 'Bias Tespit Edildi']])
                
                with col10:
                    st.markdown("**Azaltma SonrasÄ± Disparate Impact:**")
                    st.dataframe(after_di[['Grup', 'Disparate Impact', 'Bias Tespit Edildi']])
                
                # Veri setini gÃ¼ncelle
                st.success(f"Bias azaltma baÅŸarÄ±yla uygulandÄ±. Ä°ÅŸe alÄ±m oranlarÄ±ndaki fark azaltÄ±ldÄ±.")
                st.info("Sonraki analizler bias azaltma uygulanmÄ±ÅŸ veri Ã¼zerinde yapÄ±lacak.")
                df = transformed_df  # Veri setini gÃ¼ncelle
            else:
                st.error(error_msg)
        
        # Bias Azaltma Ã–nerileri
        st.subheader("Ã–nerilen DÃ¼zeltici Ã–nlemler")
        
        if di_results['Bias Tespit Edildi'].any():
            st.markdown("""
            ### YapÄ±labilecek Ä°yileÅŸtirmeler:
            
            1. **Veri Toplama ve Ä°ÅŸlemede DÃ¼zeltmeler**:
               - CV'lerden demografik bilgileri Ã§Ä±karÄ±n (cinsiyet, yaÅŸ, resim)
               - Ä°sim bilgisini anonimleÅŸtirin
               - Bias kontrollÃ¼ veri toplama sÃ¼reÃ§leri oluÅŸturun
            
            2. **Model GeliÅŸtirmede Bias Azaltma**:
               - Pre-processing: Reweighing, Disparate Impact Remover
               - In-processing: Adversarial Debiasing, Prejudice Remover
               - Post-processing: Calibrated Equalized Odds, Reject Option Classification
            
            3. **Ä°nsan GÃ¶zetimi ve SÃ¼reÃ§ Ä°yileÅŸtirmeleri**:
               - EÄŸitimli kiÅŸiler tarafÄ±ndan nihai kontrol
               - Ã‡eÅŸitlilik ekibi tarafÄ±ndan dÃ¼zenli bias denetimleri
               - Adil iÅŸe alÄ±m sÃ¼reÃ§leri hakkÄ±nda eÄŸitim
            """)
        else:
            st.info("Disparate Impact deÄŸerleri kabul edilebilir aralÄ±kta. Yine de dÃ¼zenli bias denetimleri yapmanÄ±z Ã¶nerilir.")

    # Model Analizi Sekmesi
    with tabs[2]:
        st.header("ğŸ¤– Model Analizi")
        
        # Model eÄŸitimi ve deÄŸerlendirme
        st.subheader("Model PerformansÄ±")
        
        try:
            # Model eÄŸitim ve deÄŸerlendirme
            model, X_test, y_test, y_pred, accuracy, cm, report, group_performance = train_model(df, protected_attribute, target_col)
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.info(f"""
                **Model TÃ¼rÃ¼**: {model_type}
                **Genel DoÄŸruluk**: {accuracy:.4f}
                **Precision**: {report['1']['precision']:.4f}
                **Recall**: {report['1']['recall']:.4f}
                **F1 Skoru**: {report['1']['f1-score']:.4f}
                """)
                
                # Confusion Matrix
                st.subheader("Confusion Matrix")
                fig, ax = plt.subplots(figsize=(8, 6))
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
                ax.set_xlabel('Tahmin')
                ax.set_ylabel('GerÃ§ek')
                ax.set_xticklabels(['Reddedilen', 'Ä°ÅŸe AlÄ±nan'])
                ax.set_yticklabels(['Reddedilen', 'Ä°ÅŸe AlÄ±nan'])
                st.pyplot(fig)
            
            with col2:
                # Gruplar ArasÄ± Performans
                st.subheader("Demografik Gruplar ArasÄ± Performans")
                
                group_df = pd.DataFrame.from_dict(group_performance, orient='index')
                group_df = group_df.reset_index().rename(columns={'index': protected_attribute})
                
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.barplot(x=protected_attribute, y='accuracy', data=group_df, ax=ax)
                ax.set_title(f"{protected_attribute.capitalize()} GruplarÄ±na GÃ¶re Model DoÄŸruluÄŸu")
                ax.set_xlabel(protected_attribute.capitalize())
                ax.set_ylabel("DoÄŸruluk")
                for i, v in enumerate(group_df['accuracy']):
                    ax.text(i, v + 0.02, f"{v:.2f}", ha='center')
                st.pyplot(fig)
            
            # Ã–zellik Ã–nemliliÄŸi
            st.subheader("Ã–zellik Ã–nemliliÄŸi")
            
            feature_importance = analyze_feature_importance(model, X_test)
            
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.barplot(x='importance', y='feature', data=feature_importance.head(10), ax=ax)
            ax.set_title("En Ã–nemli 10 Ã–zellik")
            ax.set_xlabel("Ã–nem Derecesi")
            ax.set_ylabel("Ã–zellik")
            st.pyplot(fig)
            
            # Microsoft Fairlearn metrikleri (eÄŸer mevcutsa)
            if fairlearn_available:
                st.subheader("Microsoft Fairlearn Metrikleri")
                
                try:
                    # Sensitive attribute
                    A = df[protected_attribute]
                    
                    # Calculate fairness metrics
                    dpd = demographic_parity_difference(y_true=y_test, 
                                                    y_pred=y_pred, 
                                                    sensitive_features=A.loc[X_test.index])
                    
                    eod = equalized_odds_difference(y_true=y_test, 
                                                y_pred=y_pred, 
                                                sensitive_features=A.loc[X_test.index])
                    
                    st.info(f"""
                    **Demographic Parity Difference**: {dpd:.4f}  
                    (0'a yakÄ±n olmasÄ± daha adil)
                    
                    **Equalized Odds Difference**: {eod:.4f}  
                    (0'a yakÄ±n olmasÄ± daha adil)
                    """)
                    
                except Exception as e:
                    st.warning(f"Fairlearn metrikleri hesaplanÄ±rken hata oluÅŸtu: {str(e)}")
            
            else:
                st.warning("Microsoft Fairlearn metrikleri iÃ§in kÃ¼tÃ¼phane yÃ¼klenmemiÅŸ.")
            
            # Model Ä°yileÅŸtirme Ã–nerileri
            st.subheader("Model Ä°yileÅŸtirme Ã–nerileri")
            
            # En bÃ¼yÃ¼k doÄŸruluk farkÄ±
            if len(group_performance) > 1:
                max_accuracy = max([gp['accuracy'] for gp in group_performance.values()])
                min_accuracy = min([gp['accuracy'] for gp in group_performance.values()])
                accuracy_diff = max_accuracy - min_accuracy
                
                if accuracy_diff > 0.1:  # %10'dan fazla fark varsa
                    st.warning(f"""
                    âš ï¸ Gruplar arasÄ±nda Ã¶nemli performans farkÄ± tespit edildi ({accuracy_diff:.2f}).
                    Bu durum, modelin bazÄ± demografik gruplar iÃ§in daha az doÄŸru tahminler yaptÄ±ÄŸÄ±nÄ± gÃ¶sterir.
                    """)
                    
                    st.markdown("""
                    ### Ä°yileÅŸtirme Ã–nerileri:
                    
                    1. **Veri Dengesi**:
                       - DezavantajlÄ± grup iÃ§in daha fazla veri toplayÄ±n
                       - Veri augmentasyonu veya resampling yÃ¶ntemleri kullanÄ±n
                    
                    2. **Ã–zellik MÃ¼hendisliÄŸi**:
                       - DezavantajlÄ± grup iÃ§in Ã¶nemli Ã¶zellikleri belirleyin
                       - Yeni, bias azaltÄ±cÄ± Ã¶zellikler ekleyin
                    
                    3. **Model SeÃ§imi ve Hiperparametre Optimizasyonu**:
                       - FarklÄ± model tÃ¼rlerini deneyin
                       - Fairness kÄ±sÄ±tlarÄ± ile hiperparametre optimizasyonu yapÄ±n
                    
                    4. **Fairness Aware Ã–ÄŸrenme**:
                       - Adversarial Debiasing gibi fairness-aware modeller kullanÄ±n
                       - Post-processing yÃ¶ntemleri ile model Ã§Ä±ktÄ±larÄ±nÄ± kalibre edin
                    """)
                else:
                    st.success(f"""
                    âœ… Gruplar arasÄ±ndaki performans farkÄ± kabul edilebilir dÃ¼zeyde ({accuracy_diff:.2f}).
                    Model, farklÄ± demografik gruplar iÃ§in benzer doÄŸrulukta tahminler yapÄ±yor.
                    """)
        
        except Exception as e:
            st.error(f"Model analizi sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}")

    # AÃ§Ä±klanabilirlik Sekmesi
    with tabs[3]:
        st.header("ğŸ” Model AÃ§Ä±klanabilirliÄŸi")
        
        if not explainability_available:
            st.warning("AÃ§Ä±klanabilirlik analizleri iÃ§in SHAP, LIME veya ELI5 kÃ¼tÃ¼phaneleri gereklidir.")
        elif 'model' not in locals():
            st.warning("AÃ§Ä±klanabilirlik analizleri iÃ§in Ã¶nce Model Analizi sekmesinde model eÄŸitimi yapÄ±lmalÄ±dÄ±r.")
        else:
            # SHAP DeÄŸerleri
            if "SHAP" in explainability_method:
                st.subheader("SHAP DeÄŸerleri Analizi")
                
                try:
                    shap_values, explainer, error_msg = generate_shap_values(model, X_test)
                    
                    if shap_values is not None:
                        col1, col2 = st.columns([1, 1])
                        
                        with col1:
                            # SHAP Ã¶zet grafiÄŸi
                            st.write("SHAP Ã–zet GrafiÄŸi (Genel Ã–zellik Etkileri)")
                            fig, ax = plt.subplots(figsize=(10, 8))
                            shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
                            st.pyplot(fig)
                        
                        with col2:
                            # SHAP baÄŸÄ±mlÄ±lÄ±k grafiÄŸi
                            st.write("SHAP BaÄŸÄ±mlÄ±lÄ±k GrafiÄŸi (Ä°ÅŸe AlÄ±m Tahmini)")
                            most_imp_feature = feature_importance['feature'].iloc[0]
                            fig, ax = plt.subplots(figsize=(10, 8))
                            shap.dependence_plot(most_imp_feature, shap_values[1], X_test, show=False)
                            st.pyplot(fig)
                        
                        # SHAP Kararlar
                        st.subheader("SHAP DeÄŸerleri ile KararlarÄ±n AÃ§Ä±klanmasÄ±")
                        
                        sample_idx = np.random.choice(len(X_test))
                        sample_instance = X_test.iloc[sample_idx]
                        sample_prediction = model.predict([sample_instance])[0]
                        
                        st.write(f"""
                        **Ã–rnek Aday**:
                        - Tahmin: {"Ä°ÅŸe AlÄ±ndÄ±" if sample_prediction == 1 else "Reddedildi"}
                        - GerÃ§ek: {"Ä°ÅŸe AlÄ±ndÄ±" if y_test.iloc[sample_idx] == 1 else "Reddedildi"}
                        """)
                        
                        # Waterfall grafiÄŸi
                        st.write("Karar AÃ§Ä±klamasÄ± (Waterfall GrafiÄŸi)")
                        fig, ax = plt.subplots(figsize=(12, 8))
                        shap.plots._waterfall.waterfall_legacy(explainer.expected_value[1], 
                                                        shap_values[1][sample_idx], 
                                                        feature_names=X_test.columns,
                                                        show=False)
                        st.pyplot(fig)
                    else:
                        st.error(error_msg)
                
                except Exception as e:
                    st.error(f"SHAP analizi sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}")
            
            # LIME AÃ§Ä±klamalarÄ±
            if "LIME" in explainability_method:
                st.subheader("LIME AÃ§Ä±klamalarÄ±")
                
                try:
                    lime_explainer, error_msg = prepare_lime_explainer(X_test, X_test.columns)
                    
                    if lime_explainer is not None:
                        # Rastgele bir Ã¶rnek seÃ§
                        sample_idx = np.random.choice(len(X_test))
                        sample_instance = X_test.iloc[sample_idx]
                        prediction = model.predict_proba([sample_instance])[0]
                        
                        st.write(f"""
                        **Ã–rnek Aday**:
                        - Ä°ÅŸe AlÄ±nma OlasÄ±lÄ±ÄŸÄ±: {prediction[1]:.2f}
                        - Tahmin: {"Ä°ÅŸe AlÄ±ndÄ±" if prediction[1] > 0.5 else "Reddedildi"}
                        """)
                        
                        # LIME aÃ§Ä±klamasÄ± oluÅŸtur
                        explanation = lime_explainer.explain_instance(
                            sample_instance.values, 
                            model.predict_proba,
                            num_features=10
                        )
                        
                        # AÃ§Ä±klamayÄ± gÃ¶rselleÅŸtir
                        st.write("Karar AÃ§Ä±klamasÄ± (LIME)")
                        fig = explanation.as_pyplot_figure(label=1)
                        st.pyplot(fig)
                        
                        # LIME aÃ§Ä±klamasÄ±nÄ± tablo olarak gÃ¶ster
                        st.write("Ã–zellik Etkileri (LIME)")
                        feature_values = explanation.as_list()
                        feature_table = pd.DataFrame(feature_values, columns=['Ã–zellik', 'Etki'])
                        st.dataframe(feature_table)
                    else:
                        st.error(error_msg)
                
                except Exception as e:
                    st.error(f"LIME analizi sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}")
            
            # ELI5 AÃ§Ä±klamalarÄ±
            if "ELI5" in explainability_method:
                st.subheader("ELI5 AÃ§Ä±klamalarÄ±")
                
                try:
                    # PermutationImportance ile Ã¶zellik Ã¶nemlerini hesapla
                    perm = PermutationImportance(model, random_state=42).fit(X_test, y_test)
                    
                    # ELI5 ile Ã¶zellik Ã¶nemlerini gÃ¶rselleÅŸtir
                    st.write("Ã–zellik Ã–nemleri (Permutation Importance)")
                    eli5_html = eli5.show_weights(perm, feature_names=X_test.columns.tolist())
                    st.components.v1.html(eli5_html.data, height=500)
                    
                    # Ã–rnek tahmin aÃ§Ä±klamasÄ±
                    st.write("Ã–rnek Aday iÃ§in Tahmin AÃ§Ä±klamasÄ±")
                    sample_idx = np.random.choice(len(X_test))
                    sample_instance = X_test.iloc[sample_idx]
                    
                    eli5_prediction = eli5.show_prediction(model, sample_instance, 
                                                        feature_names=X_test.columns.tolist())
                    st.components.v1.html(eli5_prediction.data, height=500)
                
                except Exception as e:
                    st.error(f"ELI5 analizi sÄ±rasÄ±nda bir hata oluÅŸtu: {str(e)}")
            
            # AÃ§Ä±klanabilirlik KarÅŸÄ±laÅŸtÄ±rmasÄ±
            if len(explainability_method) > 1:
                st.subheader("AÃ§Ä±klanabilirlik YÃ¶ntemlerinin KarÅŸÄ±laÅŸtÄ±rmasÄ±")
                
                st.markdown("""
                ### KarÅŸÄ±laÅŸtÄ±rma Tablosu
                
                | YÃ¶ntem | GÃ¼Ã§lÃ¼ YÃ¶nleri | ZayÄ±f YÃ¶nleri |
                |--------|--------------|--------------|
                | **SHAP** | - Teorik temellere dayanÄ±r (Shapley deÄŸerleri)<br>- TutarlÄ± ve kesin sonuÃ§lar<br>- KÃ¼resel ve yerel aÃ§Ä±klamalar | - Hesaplama maliyeti yÃ¼ksek<br>- KarmaÅŸÄ±k modellerde yavaÅŸ |
                | **LIME** | - HÄ±zlÄ± ve sezgisel<br>- Model-agnostik<br>- Yerel aÃ§Ä±klamalar iÃ§in ideal | - Ã–rnekleme varyansÄ± yÃ¼ksek<br>- KararsÄ±z sonuÃ§lar Ã¼retebilir |
                | **ELI5** | - KullanÄ±mÄ± kolay<br>- PermÃ¼tasyon Ã¶nem skorlarÄ± tutarlÄ±<br>- GÃ¶rsel aÃ§Ä±klamalar | - Ã–zellikler arasÄ± etkileÅŸimleri yakalayamaz<br>- Genellikle kÃ¼resel aÃ§Ä±klamalar saÄŸlar |
                
                ### Hangi Durumda Hangi YÃ¶ntem KullanÄ±lmalÄ±?
                
                - **SHAP**: Kesin sonuÃ§lar ve teorik gÃ¼vence istendiÄŸinde
                - **LIME**: HÄ±zlÄ±, yerel aÃ§Ä±klamalar gerektiÄŸinde
                - **ELI5**: Temel dÃ¼zeyde, kolay anlaÅŸÄ±lÄ±r aÃ§Ä±klamalar istendiÄŸinde
                
                Bias tespiti ve analizi iÃ§in birden fazla aÃ§Ä±klanabilirlik yÃ¶nteminin birlikte kullanÄ±lmasÄ± en iyi sonucu verir.
                """)

    # EU AI Act Rehberi Sekmesi
    with tabs[4]:
        st.header("ğŸ“œ EU AI Act Rehberi")
        
        st.markdown("""
        ## EU AI Act ve Ä°ÅŸe AlÄ±m Sistemleri
        
        **AB Yapay Zeka YasasÄ± (EU AI Act)**, Nisan 2024'te onaylanmÄ±ÅŸ ve iÅŸe alÄ±m, performans deÄŸerlendirme ve insan kaynaklarÄ± yÃ¶netimi iÃ§in kullanÄ±lan AI sistemlerini **yÃ¼ksek riskli AI uygulamalarÄ±** olarak sÄ±nÄ±flandÄ±rmaktadÄ±r (Madde 6 ve Ek III, Madde 4).
        """)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.image("https://miro.medium.com/v2/resize:fit:1400/1*Eo4IFEtZWBh9jOA_HT-6_g.png", 
                    caption="EU AI Act YÃ¼ksek Riskli AI Sistemleri SÄ±nÄ±flandÄ±rmasÄ±")
        
        with col2:
            st.markdown("""
            ### EU AI Act Riskli AI Sistemi Kategorileri:
            
            1. **YasaklanmÄ±ÅŸ AI Sistemleri**:
               - BilinÃ§altÄ± manipÃ¼lasyon
               - Hassas gruplarÄ±n istismarÄ±
               - Sosyal skor sistemleri
               - GerÃ§ek zamanlÄ± biyometrik tanÄ±mlama (istisnalar hariÃ§)
            
            2. **YÃ¼ksek Riskli AI Sistemleri** (Ä°ÅŸe alÄ±m sistemleri bu kategoridedir):
               - Kritik altyapÄ±
               - EÄŸitim ve mesleki eÄŸitim
               - Ä°stihdam, iÅŸÃ§i yÃ¶netimi, Ã¶z istihdama eriÅŸim
               - Temel Ã¶zel ve kamu hizmetlerine eriÅŸim
               - Kolluk kuvvetleri
               - GÃ¶Ã§, iltica ve sÄ±nÄ±r kontrolÃ¼
               - Adalet ve demokratik sÃ¼reÃ§ler
            
            3. **SÄ±nÄ±rlÄ± Riskli AI Sistemleri**:
               - Chatbotlar
               - Duygu tanÄ±ma sistemleri
               - Biyometrik kategorizasyon
               - Sentetik iÃ§erik oluÅŸturma (deepfakes)
            
            4. **Minimal Riskli AI Sistemleri**:
               - AI destekli video oyunlarÄ±
               - Spam filtreleri
               - DiÄŸer dÃ¼ÅŸÃ¼k riskli uygulamalar
            """)
        
        st.subheader("Ä°ÅŸe AlÄ±m Sistemleri iÃ§in Uyumluluk Gereksinimleri")
        
        col3, col4 = st.columns([1, 1])
        
        with col3:
            st.markdown("""
            ### 1. Risk YÃ¶netim Sistemi
            
            - Sistematik risk tanÄ±mlama ve deÄŸerlendirme
            - Risk azaltma Ã¶nlemleri geliÅŸtirme
            - DÃ¼zenli risk izleme ve gÃ¼ncelleme
            - Bias ve ayrÄ±mcÄ±lÄ±k risklerini belirleme
            
            ### 2. Veri ve Veri YÃ¶netiÅŸimi
            
            - YÃ¼ksek kaliteli eÄŸitim, doÄŸrulama ve test verileri
            - Uygun veri yÃ¶netiÅŸimi uygulamalarÄ±
            - Veri setlerinin uygun, temsili ve bias iÃ§ermediÄŸini saÄŸlama
            - Veri Ã¶n iÅŸleme, iÅŸleme ve deÄŸerlendirme sÃ¼reÃ§leri
            
            ### 3. Teknik DokÃ¼mantasyon
            
            - Sistem tasarÄ±mÄ± ve mimarisi
            - Algoritma Ã¶zellikleri ve karar mekanizmasÄ±
            - EÄŸitim metodolojisi ve veri seti Ã¶zellikleri
            - Model kartlarÄ± ve performans metrikleri
            - Bias Ã¶lÃ§Ã¼m ve azaltma sÃ¼reÃ§leri
            """)
        
        with col4:
            st.markdown("""
            ### 4. KayÄ±t Tutma ve ÅeffaflÄ±k
            
            - Otomatik kayÄ±t tutma (log)
            - Sistemin iÅŸleyiÅŸini izleme
            - Beklenmeyen sonuÃ§larÄ± tespit etme
            - Audit trail oluÅŸturma
            
            ### 5. Ä°nsan GÃ¶zetimi
            
            - Ä°nsan tarafÄ±ndan etkili gÃ¶zetim
            - Son kararÄ± insan tarafÄ±ndan onaylama
            - AI sistemini geÃ§ersiz kÄ±lma yetkisi
            - SonuÃ§larÄ± yorumlama ve mÃ¼dahale etme kapasitesi
            
            ### 6. DoÄŸruluk, DayanÄ±klÄ±lÄ±k ve GÃ¼venlik
            
            - Kabul edilebilir doÄŸruluk seviyesi
            - FarklÄ± demografik gruplar iÃ§in eÅŸit performans
            - Hatalara, tutarsÄ±zlÄ±klara ve siber saldÄ±rÄ±lara karÅŸÄ± dayanÄ±klÄ±lÄ±k
            - Yedekleme ve gÃ¼venlik planlarÄ±
            """)
        
        st.subheader("Bias DeÄŸerlendirme ve Azaltma")
        
        st.markdown("""
        ### EU AI Act KapsamÄ±nda Bias Ã–lÃ§Ã¼m Metrikleri
        
        | Metrik | AÃ§Ä±klama | Kabul Edilebilir AralÄ±k |
        |--------|-----------|------------------------|
        | **Disparate Impact** | DezavantajlÄ±/AvantajlÄ± grup seÃ§ilme oranÄ± | 0.8 - 1.2 |
        | **Statistical Parity Difference** | Gruplar arasÄ± seÃ§ilme oranÄ± farkÄ± | -0.1 - 0.1 |
        | **Equal Opportunity Difference** | Gruplar arasÄ± doÄŸru pozitif oranÄ± farkÄ± | -0.1 - 0.1 |
        | **Average Odds Difference** | Gruplar arasÄ± FPR ve TPR ortalamasÄ± farkÄ± | -0.1 - 0.1 |
        | **Theil Index** | Tahminlerdeki eÅŸitsizlik Ã¶lÃ§Ã¼mÃ¼ | 0 - 0.2 |
        
        ### Bias Azaltma Stratejileri
        
        1. **Pre-processing (Ä°ÅŸlem Ã–ncesi)**
           - Veri Resampling
           - Disparate Impact Remover
           - Learning Fair Representations
           - Optimized Preprocessing
        
        2. **In-processing (Ä°ÅŸlem SÄ±rasÄ±nda)**
           - Adversarial Debiasing
           - Exponentiated Gradient Reduction
           - Grid Search Reduction
           - Prejudice Remover
        
        3. **Post-processing (Ä°ÅŸlem SonrasÄ±)**
           - Equalized Odds Postprocessing
           - Calibrated Equalized Odds
           - Reject Option Classification
        """)
        
        st.subheader("Cezalar ve Uyumsuzluk Riskleri")
        
        st.info("""
        ### Ceza ve YaptÄ±rÄ±mlar
        
        EU AI Act, uyumsuzluk durumunda ciddi yaptÄ±rÄ±mlar Ã¶ngÃ¶rmektedir:
        
        - **YasaklanmÄ±ÅŸ AI Sistemleri**: 35 milyon â‚¬ veya global cironun %7'sine kadar
        - **YÃ¼ksek Riskli AI Sistemlerine Uyumsuzluk**: 15 milyon â‚¬ veya global cironun %3'Ã¼ne kadar
        - **DiÄŸer Uyumsuzluklar**: 7.5 milyon â‚¬ veya global cironun %1.5'ine kadar
        
        AyrÄ±ca:
        - AI sistemlerinin piyasadan Ã§ekilmesi
        - Faaliyet kÄ±sÄ±tlamalarÄ± 
        - Kamuya aÃ§Ä±k uyarÄ±lar
        - Operasyonel lisans iptali
        """)
        
        st.subheader("Uyumluluk Kontrol Listesi")
        
        uyumluluk_listesi = [
            "Risk deÄŸerlendirmesi yaptÄ±nÄ±z mÄ±?",
            "Veri setinizdeki potansiyel bias'larÄ± deÄŸerlendirdiniz mi?",
            "FarklÄ± demografik gruplar iÃ§in model performansÄ±nÄ± Ã¶lÃ§tÃ¼nÃ¼z mÃ¼?",
            "DokÃ¼mantasyon ve model kartlarÄ± hazÄ±rladÄ±nÄ±z mÄ±?",
            "Ä°nsan gÃ¶zetimi mekanizmasÄ± tasarladÄ±nÄ±z mÄ±?",
            "Bias azaltma stratejileri uyguladÄ±nÄ±z mÄ±?",
            "DÃ¼zenli sistem denetimi iÃ§in prosedÃ¼rler belirlediniz mi?",
            "Sistemin ÅŸeffaflÄ±ÄŸÄ± ve aÃ§Ä±klanabilirliÄŸi saÄŸlandÄ± mÄ±?",
            "KullanÄ±cÄ±/Ã§alÄ±ÅŸan bilgilendirme sÃ¼reÃ§leri oluÅŸturuldu mu?",
            "Olay yanÄ±t/mÃ¼dahale planÄ± geliÅŸtirildi mi?"
        ]
        
        for i, item in enumerate(uyumluluk_listesi):
            checked = st.checkbox(item, key=f"compliance_{i}")
            if not checked:
                st.warning(f"âš ï¸ {item} - EU AI Act uyumluluÄŸu iÃ§in gerekli")
        
        # Bitirme notu
        st.info("""
        **Not**: Bu rehber genel bilgilendirme amaÃ§lÄ±dÄ±r. GerÃ§ek bir EU AI Act uyumluluk sÃ¼reci iÃ§in hukuk danÄ±ÅŸmanlarÄ± 
        ve AI etik uzmanlarÄ±ndan destek alÄ±nmasÄ± Ã¶nerilir.
        """)

    # Deployment Sekmesi
    with tabs[5]:
        st.header("ğŸš€ Deployment Rehberi")
        
        st.markdown("""
        ## CV Bias Analiz AracÄ±nÄ±n Deployment SeÃ§enekleri
        
        Bu CV Bias Analiz aracÄ±nÄ± farklÄ± ortamlarda nasÄ±l deploy edebileceÄŸinize dair yÃ¶nergeler:
        """)
        
        tab1, tab2, tab3 = st.tabs(["Sunucu Deployment", "Cloud Deployment", "API Entegrasyonu"])
        
        with tab1:
            st.subheader("Sunucu Ãœzerinde Deployment")
            
            st.markdown("""
            ### Linux Sunucuda Deployment
            
            1. **Gerekli KÃ¼tÃ¼phaneleri YÃ¼kleyin**:
            ```bash
            pip install streamlit pandas numpy matplotlib seaborn scikit-learn PyPDF2 python-docx
            pip install aif360 fairlearn shap lime eli5
            ```
            
            2. **UygulamayÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n**:
            ```bash
            streamlit run app.py --server.port=8501 --server.address=0.0.0.0
            ```
            
            3. **SÃ¼rekli Ã‡alÄ±ÅŸmasÄ± Ä°Ã§in systemd Service OluÅŸturun**:
            ```bash
            sudo nano /etc/systemd/system/cvbias.service
            ```
            
            Service dosyasÄ± iÃ§eriÄŸi:
            ```
            [Unit]
            Description=CV Bias Analyzer
            After=network.target
            
            [Service]
            User=yourusername
            WorkingDirectory=/path/to/app
            ExecStart=/path/to/venv/bin/streamlit run app.py --server.port=8501 --server.address=0.0.0.0
            Restart=always
            RestartSec=5
            
            [Install]
            WantedBy=multi-user.target
            ```
            
            Servisi etkinleÅŸtirme:
            ```bash
            sudo systemctl enable cvbias.service
            sudo systemctl start cvbias.service
            ```
            
            4. **NGINX ile Reverse Proxy AyarlayÄ±n**:
            ```bash
            sudo apt install nginx
            sudo nano /etc/nginx/sites-available/cvbias
            ```
            
            NGINX config iÃ§eriÄŸi:
            ```
            server {
                listen 80;
                server_name your-domain.com;
                
                location / {
                    proxy_pass http://localhost:8501;
                    proxy_http_version 1.1;
                    proxy_set_header Upgrade $http_upgrade;
                    proxy_set_header Connection 'upgrade';
                    proxy_set_header Host $host;
                    proxy_cache_bypass $http_upgrade;
                }
            }
            ```
            
            Siteyi etkinleÅŸtirme:
            ```bash
            sudo ln -s /etc/nginx/sites-available/cvbias /etc/nginx/sites-enabled
            sudo systemctl restart nginx
            ```
            
            5. **SSL SertifikasÄ± Ekleyin**:
            ```bash
            sudo apt install certbot python3-certbot-nginx
            sudo certbot --nginx -d your-domain.com
            ```
            """)
        
        with tab2:
            st.subheader("Cloud Deployment SeÃ§enekleri")
            
            st.markdown("""
            ### 1. Streamlit Cloud
            
            En kolay deployment yÃ¶ntemi:
            
            1. GitHub'a kodunuzu push edin
            2. [Streamlit Cloud](https://streamlit.io/cloud) hesabÄ± oluÅŸturun
            3. New app > GitHub repo > Main file: app.py
            4. Deploy app
            
            ### 2. Heroku
            
            1. Heroku hesabÄ± oluÅŸturun
            2. requirements.txt dosyasÄ± hazÄ±rlayÄ±n
            3. Procfile oluÅŸturun: `web: streamlit run app.py --server.port=$PORT`
            4. Heroku CLI ile deploy edin:
            
            ```bash
            heroku login
            heroku create cv-bias-analyzer
            git push heroku main
            ```
            
            ### 3. AWS Elastic Beanstalk
            
            1. AWS hesabÄ± oluÅŸturun
            2. AWS CLI ve EB CLI yÃ¼kleyin
            3. Uygulama dizininde:
            
            ```bash
            eb init -p python-3.8 cv-bias-analyzer
            eb create cv-bias-env
            eb deploy
            ```
            
            ### 4. Google Cloud Run
            
            1. Dockerfile oluÅŸturun:
            
            ```Dockerfile
            FROM python:3.9-slim
            
            WORKDIR /app
            
            COPY requirements.txt .
            RUN pip install -r requirements.txt
            
            COPY . .
            
            EXPOSE 8080
            
            CMD streamlit run app.py --server.port=8080 --server.address=0.0.0.0
            ```
            
            2. GCP CLI ile deploy edin:
            
            ```bash
            gcloud builds submit --tag gcr.io/[PROJECT-ID]/cv-bias-analyzer
            gcloud run deploy --image gcr.io/[PROJECT-ID]/cv-bias-analyzer --platform managed
            ```
            """)
        
        with tab3:
            st.subheader("API Entegrasyonu")
            
            st.markdown("""
            ### CV Bias Analiz API'si OluÅŸturma
            
            Streamlit uygulamasÄ± yerine, sisteminizi bir API olarak da sunabilirsiniz:
            
            1. **FastAPI ile API OluÅŸturma**:
            
            ```python
            from fastapi import FastAPI, File, UploadFile
            import pandas as pd
            import io
            
            app = FastAPI(title="CV Bias Analyzer API")
            
            @app.post("/analyze-bias/")
            async def analyze_bias(file: UploadFile = File(...), 
                                 protected_attribute: str = "cinsiyet",
                                 target_col: str = "ise_alindi"):
                # CSV dosyasÄ±nÄ± oku
                content = await file.read()
                df = pd.read_csv(io.StringIO(content.decode('utf-8')))
                
                # Bias analizi yap
                results = calculate_disparate_impact(df, protected_attribute, target_col)
                
                return {
                    "bias_detected": results['Bias Tespit Edildi'].any(),
                    "metrics": results.to_dict(orient="records")
                }
            
            @app.post("/mitigate-bias/")
            async def mitigate_bias(file: UploadFile = File(...),
                                  protected_attribute: str = "cinsiyet",
                                  target_col: str = "ise_alindi",
                                  method: str = "reweighing"):
                # Bias azaltma iÅŸlemleri
                # ...
                
                return {"status": "success", "method": method}
            ```
            
            2. **Docker ile API Deployment**:
            
            ```Dockerfile
            FROM python:3.9-slim
            
            WORKDIR /app
            
            COPY requirements.txt .
            RUN pip install -r requirements.txt
            
            COPY . .
            
            EXPOSE 8000
            
            CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
            ```
            
            3. **API Gateway ve Lambda ile Serverless Deployment**:
            
            AWS Lambda ile API'yi serverless olarak sunabilirsiniz:
            
            ```python
            def lambda_handler(event, context):
                # API Gateway'den gelen request'i iÅŸle
                # Bias analizi yap
                # SonuÃ§larÄ± dÃ¶ndÃ¼r
                return {
                    'statusCode': 200,
                    'body': json.dumps({
                        'bias_detected': True,
                        'metrics': [...]
                    })
                }
            ```
            
            4. **Mevcut Sistemlere Entegrasyon**:
            
            Bias analiz API'nizi ÅŸu sistemlere entegre edebilirsiniz:
            
            - ATS (Applicant Tracking Systems)
            - Ä°nsan KaynaklarÄ± YÃ¶netim Sistemleri
            - Ã–zgeÃ§miÅŸ Tarama YazÄ±lÄ±mlarÄ±
            - Ä°ÅŸ Ä°lanÄ± PlatformlarÄ±
            - Åirket Ä°Ã§i Talent Management Sistemleri
            """)
        
        st.subheader("VerifyWise Platformu Entegrasyonu")
        
        st.markdown("""
        ## VerifyWise ile Entegrasyon
        
        CV Bias Analiz aracÄ±nÄ± VerifyWise platformuna entegre etmek iÃ§in Ã¶nerilen adÄ±mlar:
        
        ### 1. Teknik Entegrasyon
        
        - **API TabanlÄ± Entegrasyon**: CV Bias analiz motorunu RESTful API olarak sunun
        - **SDK GeliÅŸtirme**: VerifyWise iÃ§in Ã¶zel Python/JavaScript SDK hazÄ±rlayÄ±n
        - **Webhook DesteÄŸi**: GerÃ§ek zamanlÄ± bias uyarÄ±larÄ± iÃ§in webhook mekanizmasÄ± ekleyin
        
        ### 2. Veri AkÄ±ÅŸÄ±
        
        - VerifyWise platformundan gelen CV'leri otomatik analiz edin
        - Bias sonuÃ§larÄ±nÄ± ve metrikleri VerifyWise dashboard'una gÃ¶nderin
        - Periyodik bias raporlarÄ±nÄ± otomatik oluÅŸturun
        
        ### 3. Ã–zel VerifyWise ModÃ¼lleri
        
        - **Bias MonitÃ¶r Dashboard**: Zaman iÃ§indeki bias metriklerini izleme
        - **EU AI Act Uyumluluk Paneli**: DÃ¼zenleyici gereksinimleri takip etme
        - **DÃ¼zeltici Aksiyon Ã–neri ModÃ¼lÃ¼**: Tespit edilen bias'larÄ± azaltma Ã¶nerileri
        
        ### 4. Ã–lÃ§eklenebilirlik
        
        - Microservice mimarisi ile componentlarÄ± ayÄ±rÄ±n
        - Kubernetes ile container orchestration saÄŸlayÄ±n
        - Auto-scaling ile yÃ¼ksek trafik dÃ¶nemlerinde performansÄ± koruyun
        
        ### Ã–nerilen Zaman Ã‡izelgesi
        
        | AÅŸama | SÃ¼re | Aktiviteler |
        |-------|------|-------------|
        | **PoC** | 4 Hafta | Temel bias analiz motorunun geliÅŸtirilmesi |
        | **MVP** | 8 Hafta | API entegrasyonu ve ilk VerifyWise modÃ¼lÃ¼ |
        | **Beta** | 12 Hafta | Tam EU AI Act uyumluluÄŸu ve test sÃ¼rÃ¼mÃ¼ |
        | **Lansman** | 16 Hafta | Production ortamÄ±na geÃ§iÅŸ ve ilk mÃ¼ÅŸterilere sunum |
        """)
        
        st.success("""
        VerifyWise platformuyla entegre edilmiÅŸ CV Bias Analiz aracÄ±, ÅŸirketlerin EU AI Act uyumluluÄŸunu saÄŸlamalarÄ±na 
        yardÄ±mcÄ± olurken, aynÄ± zamanda daha adil iÅŸe alÄ±m sÃ¼reÃ§leri oluÅŸturmalarÄ±nÄ± saÄŸlayacaktÄ±r.
        """)
